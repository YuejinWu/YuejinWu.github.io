<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自注意力机制_简读</title>
      <link href="/2025/12/16/self-attention%E7%AE%80%E8%AF%BB/"/>
      <url>/2025/12/16/self-attention%E7%AE%80%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="引言：自注意力——序列建模的新范式"><a href="#引言：自注意力——序列建模的新范式" class="headerlink" title="引言：自注意力——序列建模的新范式"></a>引言：自注意力——序列建模的新范式</h2><p>在深度学习的发展历程中，序列建模一直是极具挑战性的任务。从早期的循环神经网络（RNN）到长短期记忆网络（LSTM），研究者们不断探索如何有效捕捉序列数据中的依赖关系。然而，这些模型都存在一个根本性局限：<strong>顺序处理机制</strong>导致计算无法并行化，且难以建模长距离依赖。</p><p>自注意力（Self-Attention）机制的提出彻底改变了这一局面。它不仅解决了长距离依赖问题，还实现了高效的并行计算，成为自然语言处理、计算机视觉乃至语音处理等领域的核心技术。</p><p>本文将深入剖析自注意力机制的多个关键方面，包括其如何编码位置信息、如何应对超长序列、以及与卷积神经网络和循环神经网络的对比，为你呈现一个完整的自注意力知识图谱。</p><h2 id="位置编码：为无位置感知的自注意力注入顺序信息"><a href="#位置编码：为无位置感知的自注意力注入顺序信息" class="headerlink" title="位置编码：为无位置感知的自注意力注入顺序信息"></a>位置编码：为无位置感知的自注意力注入顺序信息</h2><h3 id="问题的本质：自注意力的“位置盲”"><a href="#问题的本质：自注意力的“位置盲”" class="headerlink" title="问题的本质：自注意力的“位置盲”"></a>问题的本质：自注意力的“位置盲”</h3><p>自注意力机制有一个根本特性：<strong>它对输入序列中元素的绝对位置或相对顺序没有内在感知</strong>。这是因为自注意力通过查询（Query）、键（Key）和值（Value）的交互计算注意力权重时，仅考虑内容相似性，而不考虑位置关系。</p><p>这就造成了一个有趣的现象：对于自注意力层来说，序列中第一个位置和最后一个位置在计算上是完全平等的——“天涯若比邻”，所有位置之间的距离在计算上没有差别。但在许多实际任务中，位置信息至关重要。例如在词性标注任务中，动词很少出现在句首；在语音识别中，声音信号的时序关系是关键线索。</p><h3 id="解决方案：将位置信息显式编码"><a href="#解决方案：将位置信息显式编码" class="headerlink" title="解决方案：将位置信息显式编码"></a>解决方案：将位置信息显式编码</h3><p>为了解决这一问题，研究者们提出了<strong>位置编码（Positional Encoding）</strong>。其核心思想是为序列中的每个位置分配一个独特的向量表示，然后将这个位置向量与对应位置的输入向量相加：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入 = 内容向量 + 位置向量</span><br></pre></td></tr></table></figure><p>这样，自注意力机制在计算时就能同时考虑内容信息和位置信息。</p><h3 id="位置编码的实现方式"><a href="#位置编码的实现方式" class="headerlink" title="位置编码的实现方式"></a>位置编码的实现方式</h3><p>位置编码有多种实现方法：</p><ol><li><p><strong>正弦&#x2F;余弦函数编码</strong>：这是Transformer原始论文采用的方法，使用不同频率的正弦和余弦函数生成位置向量：</p><ul><li><p>优点：可以处理任意长度的序列，且能捕捉相对位置关系</p></li><li><p>原理：不同维度使用不同频率的三角函数，使模型能够学习到相对位置关系</p></li></ul></li><li><p><strong>可学习的位置编码</strong>：将位置编码作为可训练参数，随模型一同学习</p><ul><li><p>优点：更加灵活，能够适应特定任务的需求</p></li><li><p>缺点：无法处理训练时未见过的序列长度</p></li></ul></li><li><p><strong>相对位置编码</strong>：关注元素之间的相对距离而非绝对位置</p><ul><li><p>优点：更好地泛化到不同长度的序列</p></li><li><p>应用：在诸如Transformer-XL等改进模型中使用</p></li></ul></li></ol><p>位置编码的研究仍在继续，最新研究如《Learning to Encode Position for Transformer with Continuous Dynamical Model》探索了更动态、更灵活的位置编码方法。</p><h2 id="截断自注意力：应对超长序列的计算挑战"><a href="#截断自注意力：应对超长序列的计算挑战" class="headerlink" title="截断自注意力：应对超长序列的计算挑战"></a>截断自注意力：应对超长序列的计算挑战</h2><h3 id="问题的根源：二次方复杂度"><a href="#问题的根源：二次方复杂度" class="headerlink" title="问题的根源：二次方复杂度"></a>问题的根源：二次方复杂度</h3><p>标准自注意力机制有一个显著的计算瓶颈：<strong>计算复杂度与序列长度的二次方成正比</strong>。具体来说，对于一个长度为L的序列，自注意力需要计算L×L的注意力矩阵，这在序列很长时会带来巨大的计算和内存开销。</p><p>例如在语音处理任务中，1秒的声音信号可能对应100个向量，5秒就是500个向量，普通的一句话就可能包含数千个向量。如此长的序列会使注意力矩阵变得极其庞大，难以处理和训练。</p><h3 id="截断自注意力：局部关注的智慧"><a href="#截断自注意力：局部关注的智慧" class="headerlink" title="截断自注意力：局部关注的智慧"></a>截断自注意力：局部关注的智慧</h3><p>截断自注意力（Truncated Self-Attention）是一种有效的解决方案。其核心思想是：<strong>让每个位置只关注其周围有限范围内的其他位置，而不是整个序列</strong>。</p><p>这种设计基于一个合理的假设：在许多任务中，一个元素的语义主要受其邻近元素影响。例如在语音识别中，要识别某个位置的音素，通常只需要考虑其前后一定时间范围内的声音信号即可。</p><h3 id="实现方式与优势"><a href="#实现方式与优势" class="headerlink" title="实现方式与优势"></a>实现方式与优势</h3><p>截断自注意力通过限制每个查询向量只能与一定窗口内的键向量交互来实现：</p><ul><li><p><strong>固定窗口</strong>：每个位置只关注前后k个位置</p></li><li><p><strong>动态窗口</strong>：根据内容动态决定关注范围</p></li><li><p><strong>分层注意力</strong>：结合不同粒度的注意力，先粗后细</p></li></ul><p>截断自注意力的主要优势包括：</p><ol><li><p>将计算复杂度从O(L²)降低到O(L×k)，其中k是窗口大小</p></li><li><p>大幅减少内存消耗</p></li><li><p>在某些任务上可能提高性能，因为避免了无关远程信息的干扰</p></li></ol><h2 id="自注意力与卷积神经网络：两种视觉架构的对比"><a href="#自注意力与卷积神经网络：两种视觉架构的对比" class="headerlink" title="自注意力与卷积神经网络：两种视觉架构的对比"></a>自注意力与卷积神经网络：两种视觉架构的对比</h2><h3 id="图像作为向量序列：统一的数据视角"><a href="#图像作为向量序列：统一的数据视角" class="headerlink" title="图像作为向量序列：统一的数据视角"></a>图像作为向量序列：统一的数据视角</h3><p>传统上，图像处理是卷积神经网络（CNN）的领域。但如果我们换个视角，将图像视为一个向量序列，就能用自注意力处理图像任务：</p><p>一张H×W×C的图像可以看作H×W个C维向量组成的序列，其中每个向量对应一个像素（或一个图像块）。基于这种表示，自注意力可以直接应用于图像数据，相关研究包括《Self-Attention Generative Adversarial Networks》和《End-to-End Object Detection with Transformers》。</p><h3 id="感受野：人工划定与自动学习"><a href="#感受野：人工划定与自动学习" class="headerlink" title="感受野：人工划定与自动学习"></a>感受野：人工划定与自动学习</h3><p>卷积神经网络和自注意力在如何处理图像信息上有着根本区别：</p><ul><li><p><strong>卷积神经网络</strong>：每个神经元只处理局部感受野内的信息，感受野的大小和形状是<strong>人工预设</strong>的</p></li><li><p><strong>自注意力</strong>：每个位置可以与图像中任何其他位置交互，有效感受野是<strong>自动学习</strong>的</p></li></ul><p>这种差异带来一个有趣的结论：<strong>卷积神经网络实际上是自注意力的一个特例</strong>。当自注意力被约束为只关注局部区域，并且权重共享时，它就退化成了卷积操作。这一观点在论文《On the Relationship between Self-attention and Convolutional Layers》中得到了数学上的严格证明。</p><h3 id="灵活性与数据需求的平衡"><a href="#灵活性与数据需求的平衡" class="headerlink" title="灵活性与数据需求的平衡"></a>灵活性与数据需求的平衡</h3><p>自注意力的灵活性既是优势也是挑战：</p><ul><li><p><strong>灵活性</strong>：自注意力能够自适应地学习不同位置之间的关系模式，不受固定几何结构的限制</p></li><li><p><strong>数据需求</strong>：更高的灵活性意味着更大的假设空间，需要更多数据来学习有效的模式</p></li></ul><p>谷歌的研究《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》清楚地展示了这一权衡：</p><ul><li><p>在小规模数据（1000万张图像）上，CNN表现更好</p></li><li><p>在大规模数据（3亿张图像）上，Vision Transformer超越CNN</p></li></ul><p>这一现象揭示了深度学习中一个普遍规律：<strong>更灵活的模型需要更多数据来发挥潜力</strong>。在实际应用中，可以根据数据量的大小选择合适的架构，或者将两者结合（如Conformer模型同时使用自注意力和卷积）。</p><h2 id="自注意力与循环神经网络：序列建模的范式转变"><a href="#自注意力与循环神经网络：序列建模的范式转变" class="headerlink" title="自注意力与循环神经网络：序列建模的范式转变"></a>自注意力与循环神经网络：序列建模的范式转变</h2><h3 id="处理序列的两种哲学"><a href="#处理序列的两种哲学" class="headerlink" title="处理序列的两种哲学"></a>处理序列的两种哲学</h3><p>循环神经网络（RNN）和自注意力代表了序列建模的两种不同哲学：</p><ul><li><p><strong>RNN的迭代哲学</strong>：逐步处理序列，将历史信息压缩到隐藏状态中，具有内在的顺序性</p></li><li><p><strong>自注意力的并行哲学</strong>：同时处理所有位置，通过注意力机制直接建立任意两个位置的联系</p></li></ul><p>即使是最先进的双向RNN，要捕捉长距离依赖也需要将信息“记忆”并传递很长距离，而自注意力可以“天涯若比邻”，直接建立远程连接。</p><h3 id="并行化：效率的革命性提升"><a href="#并行化：效率的革命性提升" class="headerlink" title="并行化：效率的革命性提升"></a>并行化：效率的革命性提升</h3><p>自注意力相对于RNN最显著的优势是<strong>完全并行化</strong>：</p><ul><li><p>RNN必须顺序处理序列，每一步都依赖前一步的输出</p></li><li><p>自注意力可以同时计算所有位置的表示，极大提高了计算效率</p></li></ul><p>这一差异在现代硬件（尤其是GPU&#x2F;TPU）上具有重大意义，使得训练大规模序列模型成为可能。</p><h3 id="信息流动的差异"><a href="#信息流动的差异" class="headerlink" title="信息流动的差异"></a>信息流动的差异</h3><p>两种架构在信息流动路径上也存在根本差异：</p><ul><li><p>RNN的信息流动是<strong>序列化的</strong>，信息从早期位置流向后期位置需要经过多个时间步</p></li><li><p>自注意力的信息流动是<strong>全连接的</strong>，任何两个位置都可以直接交互</p></li></ul><p>这使得自注意力特别适合建模复杂的长距离依赖，而这是许多序列任务（如机器翻译、文档理解）的关键。</p><h2 id="自注意力在图数据上的应用"><a href="#自注意力在图数据上的应用" class="headerlink" title="自注意力在图数据上的应用"></a>自注意力在图数据上的应用</h2><p>图数据（节点和边组成的结构）也可以使用自注意力处理，此时需要做适当调整：</p><ol><li><p><strong>节点作为向量</strong>：图中的每个节点可以表示为一个向量</p></li><li><p><strong>边作为注意力约束</strong>：已有的边信息可以作为先验，约束注意力计算只发生在相连的节点之间</p></li><li><p><strong>图神经网络作为受限自注意力</strong>：这种受限的自注意力本质上就是图神经网络（GNN）的一种形式</p></li></ol><p>这种方法的优势在于结合了数据驱动的关系学习和先验的结构信息，在社交网络分析、分子结构建模等领域有广泛应用。</p><h2 id="自注意力的变体与未来方向"><a href="#自注意力的变体与未来方向" class="headerlink" title="自注意力的变体与未来方向"></a>自注意力的变体与未来方向</h2><p>随着研究的深入，出现了多种自注意力变体，旨在解决标准自注意力的各种限制：</p><ol><li><p><strong>效率优化型</strong>：如Linformer、Performer、Reformer等，通过数学近似或架构改进降低计算复杂度</p></li><li><p><strong>长序列专用型</strong>：如Longformer、BigBird等，专门处理极长序列</p></li><li><p><strong>领域适配型</strong>：针对特定任务或数据类型的定制化注意力机制</p></li></ol><p>《Long Range Arena: A Benchmark for Efficient Transformers》系统比较了各种自注意力变体，《Efficient Transformers: A Survey》则全面综述了这一领域的发展。</p><h2 id="结语：自注意力——深度学习的通用构件"><a href="#结语：自注意力——深度学习的通用构件" class="headerlink" title="结语：自注意力——深度学习的通用构件"></a>结语：自注意力——深度学习的通用构件</h2><p>自注意力机制已经从自然语言处理领域的一个创新，发展成为深度学习的通用构件。它的核心价值在于：</p><ol><li><p><strong>灵活性</strong>：能够适应各种类型的数据和任务</p></li><li><p><strong>可并行性</strong>：充分利用现代计算硬件</p></li><li><p><strong>长距离建模能力</strong>：直接建立远程依赖关系</p></li><li><p><strong>可解释性</strong>：注意力权重提供了模型决策的透明视图</p></li></ol><p>然而，自注意力并非万能钥匙。它的高计算复杂度、对大规模数据的依赖以及在某些任务上的次优表现，都是需要继续研究的问题。未来的方向可能包括更高效的注意力机制、更好的位置编码方法，以及与其他架构（如CNN、GNN）的更深度融合。</p><p>自注意力的发展历程体现了深度学习研究的一个核心理念：<strong>寻找既强大又高效的通用归纳偏置</strong>。它不仅仅是一个技术工具，更是一种重新思考序列建模、关系建模乃至结构化数据建模的新范式。随着研究的不断深入，我们有理由相信，自注意力及其变体将继续推动人工智能技术的发展边界。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Self Attention </tag>
            
            <tag> 自注意力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM简读</title>
      <link href="/2025/12/16/RNN%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/12/16/RNN%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="本质与起源"><a href="#本质与起源" class="headerlink" title="本质与起源"></a>本质与起源</h2><p>长短期记忆网络（Long Short-Term Memory，LSTM）是一种特殊的循环神经网络架构，其根本目的在于解决传统RNN在处理长序列数据时面临的<strong>长期依赖学习难题</strong>。这个问题的核心在于：当序列长度增加时，信息在时间维度上反向传播时会出现<strong>梯度动力学失衡</strong>——梯度要么指数级衰减至近乎消失，无法影响早期时间步的参数更新；要么指数级增长引发数值爆炸，导致训练崩溃。</p><p>LSTM的哲学智慧在于：<strong>将信息存储与信息处理分离</strong>。它通过引入一个独立且受保护的记忆通道，使得信息能够在时间轴上稳定传输，同时通过可学习的门控机制来决定信息的进出与留存，从而模拟人类记忆的“选择性记忆”与“选择性遗忘”过程。</p><h2 id="核心架构的深度剖析"><a href="#核心架构的深度剖析" class="headerlink" title="核心架构的深度剖析"></a>核心架构的深度剖析</h2><h3 id="记忆元（Memory-Cell）：信息的持久化载体"><a href="#记忆元（Memory-Cell）：信息的持久化载体" class="headerlink" title="记忆元（Memory Cell）：信息的持久化载体"></a>记忆元（Memory Cell）：信息的持久化载体</h3><p>记忆元是LSTM的灵魂所在，它是一个贯穿时间序列的连续状态流。与传统RNN中隐藏状态不断被覆写不同，记忆元的设计允许信息以<strong>累积而非替换</strong>的方式存在。这种设计带来了两个关键优势：</p><ol><li><p><strong>梯度流的持续性</strong>：由于记忆元的更新是加法操作（旧状态的部分保留加上新输入的部分添加），梯度在反向传播时可以通过记忆元路径稳定流动，避免了传统RNN中因矩阵连乘导致的梯度指数衰减或爆炸。</p></li><li><p><strong>信息生命周期管理</strong>：记忆元中的信息可以理论上无限期保存，除非被主动遗忘。这使网络能够捕捉跨越数百甚至数千时间步的依赖关系。</p></li></ol><h3 id="门控机制的三位一体"><a href="#门控机制的三位一体" class="headerlink" title="门控机制的三位一体"></a>门控机制的三位一体</h3><p>LSTM的精妙之处在于它的三个门，每个门都是一个独立的神经网络层，使用sigmoid激活函数将输入映射到[0,1]区间，实现对信息流的<strong>精细化模拟调控</strong>。</p><p><strong>遗忘门（Forget Gate）</strong> 控制历史记忆的留存比例。其设计哲学是：并非所有过去信息都值得保留。门输出值接近1表示“完全记住”，接近0表示“完全遗忘”。这里有一个重要的认知反直觉点：在LSTM的术语中，“遗忘门开启”意味着保留记忆，“关闭”意味着遗忘。这种设计源于sigmoid函数的输出特性，通常通过偏置初始化来设定默认行为（正偏置使门倾向于开启）。</p><p><strong>输入门（Input Gate）</strong> 控制新信息的准入程度。它决定当前时间步的候选信息有多少值得纳入长期记忆。候选信息本身由tanh函数生成，提供非线性变换并将值规范到[-1,1]范围。输入门与候选信息的逐元素相乘实现了对新信息的过滤和加权。</p><p><strong>输出门（Output Gate）</strong> 控制记忆对外的显露程度。记忆元内部状态经过tanh规范化后，由输出门调节最终暴露给外部网络的部分。这种设计使LSTM能够区分“内部存储”与“对外发布”，增加了信息处理的层次性。</p><h3 id="信息流动的完整动力学"><a href="#信息流动的完整动力学" class="headerlink" title="信息流动的完整动力学"></a>信息流动的完整动力学</h3><p>记忆元的更新遵循一个物理直观的方程：<strong>新记忆 &#x3D; 旧记忆 × 遗忘比例 + 新信息 × 输入比例</strong>。这种形式本质上是<strong>带门控的残差连接</strong>，它确保了：</p><ul><li><p>当遗忘门接近1且输入门接近0时，记忆几乎完全保留，新信息被阻挡</p></li><li><p>当遗忘门接近0且输入门接近1时，记忆被清空，新信息完全写入</p></li><li><p>在大多数中间状态，实现新旧信息的平滑融合</p></li></ul><p>隐藏状态的生成则是内部记忆的受限视图：记忆元状态经tanh压缩后，由输出门调制输出。这种分离使得LSTM可以存储不直接输出的中间信息，增加了网络的表征能力。</p><h2 id="训练动态与优化考量"><a href="#训练动态与优化考量" class="headerlink" title="训练动态与优化考量"></a>训练动态与优化考量</h2><h3 id="梯度行为的根本改善"><a href="#梯度行为的根本改善" class="headerlink" title="梯度行为的根本改善"></a>梯度行为的根本改善</h3><p>LSTM解决梯度消失问题的核心机制在于其<strong>加法更新路径</strong>。在反向传播时，梯度可以通过记忆元路径以近似常数的形式回溯，因为：</p><p>$∂c_t&#x2F;∂c_{t-1} &#x3D; f_t + (其他项)$</p><p>只要遗忘门f_t保持在合理范围（如接近1），这个导数就不会消失。相比之下，传统RNN的对应项是权重矩阵的连乘，极易导致梯度指数变化。</p><p>然而，LSTM并未完全消除梯度爆炸风险。门控信号本身的生成涉及权重矩阵乘法，这些路径仍可能出现大梯度。因此，<strong>梯度裁剪</strong>成为LSTM训练的标配技术，通过限制梯度范数防止参数更新步长过大。</p><h3 id="参数初始化策略"><a href="#参数初始化策略" class="headerlink" title="参数初始化策略"></a>参数初始化策略</h3><p>LSTM的初始状态设定深刻影响其学习行为：</p><ul><li><p>遗忘门偏置通常初始化为正数（如+1），确保初始阶段倾向于保留记忆</p></li><li><p>输入门偏置可初始化为零或负数，避免早期过度写入</p></li><li><p>输出门偏置可初始化为零或负数，控制信息释放节奏</p></li><li><p>权重矩阵常用正交初始化或Xavier初始化，保持激活值方差稳定</p></li></ul><h3 id="正则化与结构变体"><a href="#正则化与结构变体" class="headerlink" title="正则化与结构变体"></a>正则化与结构变体</h3><p>在LSTM中应用Dropout需要特别注意：<strong>时间步之间的Dropout会破坏序列依赖性</strong>，因此通常只在层间或垂直方向应用。循环Dropout（在隐藏状态间随机丢弃）需要精心设计以保持长期记忆能力。</p><p><strong>Peephole连接</strong>是LSTM的一个重要扩展，允许门控单元直接“窥视”记忆元状态。这使得门控决策不仅基于输入和前一隐藏状态，还能感知当前记忆内容，理论上增强了门控的精确性。其连接方式有三种变体：仅输入遗忘门查看前一时刻记忆，仅输出门查看当前记忆，或所有门都查看相应记忆。</p><h2 id="扩展生态与简化版本"><a href="#扩展生态与简化版本" class="headerlink" title="扩展生态与简化版本"></a>扩展生态与简化版本</h2><p><strong>多层LSTM</strong>通过堆叠多个LSTM层构建深度时序模型。深层结构能够学习不同时间尺度上的特征：底层捕捉局部模式，高层整合全局依赖。层间连接可以是简单的传递，也可以加入跳跃连接缓解优化难度。</p><p><strong>双向LSTM</strong>同时考虑过去和未来上下文，通过前向和后向两个LSTM的隐藏状态拼接，获得每个时间步的完整语境表征。这在序列标注等任务中效果显著。</p><p><strong>门控循环单元（GRU）</strong> 作为LSTM的简化版本，将输入门和遗忘门合并为<strong>更新门</strong>，并取消了独立的记忆元与隐藏状态的分离。GRU的参数减少约1&#x2F;3，训练速度更快，内存占用更少，在许多任务上与LSTM性能相当。其核心思想是“以新换旧”的更新策略：更新门同时控制旧状态的保留比例和新状态的写入比例。</p><h2 id="为什么LSTM有效：多重视角理解"><a href="#为什么LSTM有效：多重视角理解" class="headerlink" title="为什么LSTM有效：多重视角理解"></a>为什么LSTM有效：多重视角理解</h2><p><strong>信息论视角</strong>：LSTM实现了可变速率的信息通道。遗忘门控制信息衰减率，输入门控制信息获取率，输出门控制信息释放率。这三个速率参数通过数据学习自适应调整，使网络在信息保存与更新间达到动态平衡。</p><p><strong>动力学系统视角</strong>：记忆元构成一个缓慢变化的动态系统，门控机制引入快变子系统。这种时间尺度分离使网络能同时捕捉快速变化的模式特征和缓慢演变的语境信息。</p><p><strong>算法视角</strong>：LSTM本质上学习了一个可微分的记忆管理算法。它通过门控函数实现了类似“if-else”的条件逻辑，但完全可微分，能够通过梯度下降端到端优化。</p><p><strong>认知科学视角</strong>：LSTM的结构与工作记忆模型有内在相似性。记忆元类似于工作记忆的存储缓冲区，门控机制类似于注意力的分配过程，输出门类似于回忆提取过程。</p><h2 id="实践中的深刻洞察"><a href="#实践中的深刻洞察" class="headerlink" title="实践中的深刻洞察"></a>实践中的深刻洞察</h2><ol><li><p><strong>LSTM不是万能的</strong>：对于极长序列（如数千步），即使LSTM也可能出现梯度动力学问题。此时可能需要结合注意力机制或层次化处理。</p></li><li><p><strong>计算代价与收益</strong>：LSTM的4倍参数增长带来显著的表达能力和记忆容量提升，但也增加了过拟合风险。需要根据数据规模和任务复杂度权衡。</p></li><li><p><strong>时间方向的不可逆性</strong>：标准LSTM严格遵循时间因果律，这在许多时序预测任务中是优势，但也限制了某些场景下对双向上下文的同时利用。</p></li><li><p><strong>记忆与泛化的张力</strong>：LSTM的强大记忆能力可能使其过度拟合训练数据的特定时间模式，需要适当的正则化引导其学习通用时序动态。</p></li></ol><h2 id="总结升华"><a href="#总结升华" class="headerlink" title="总结升华"></a>总结升华</h2><p>LSTM的突破性贡献在于：<strong>它将记忆本身作为可学习的对象</strong>。传统RNN将记忆视为隐藏状态演化的副产品，而LSTM明确构建了一个受保护的记忆通道，并通过可微分的门控机制赋予网络管理自身记忆的能力。</p><p>这种架构创新背后的核心洞见是：<strong>长期依赖学习的关键不是避免梯度消失，而是提供梯度流动的稳定通路</strong>。LSTM通过加法更新路径和门控调制，创造了一条梯度可以无衰减回溯的“时间高速公路”，同时又不丧失对信息流动的精细控制。</p><p>更深层地看，LSTM的成功反映了神经网络设计的一个重要原则：<strong>好的归纳偏置应该编码在架构中，而非完全依赖数据学习</strong>。LSTM的架构本身编码了“信息应该被选择性地记忆和遗忘”这一关于时序数据本质的先验知识，这使得它在有限数据下也能学习有效的长期依赖。</p><p>从RNN到LSTM的演进，标志着循环神经网络从简单的时序函数逼近器，进化为具有内在记忆管理能力的复杂动态系统。这一进化不仅解决了长期依赖的技术难题，更开辟了神经网络模拟高阶认知功能的新可能。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生活感悟</title>
      <link href="/2025/12/10/%E9%97%B2%E8%A8%80%E7%A2%8E%E8%AF%AD2/"/>
      <url>/2025/12/10/%E9%97%B2%E8%A8%80%E7%A2%8E%E8%AF%AD2/</url>
      
        <content type="html"><![CDATA[<p>！！！</p>]]></content>
      
      
      <categories>
          
          <category> essay </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Yuejin的个人介绍</title>
      <link href="/2025/12/09/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/"/>
      <url>/2025/12/09/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>我是<em>Yuejin Wu</em>，目前是BUPT本科三年级学生 </p><p>专业是人工智能，还得 **考研 ** 🫥</p><p>我搭建这个<em>Yuejin’s Blog</em>，记录一下我的生活，监督一下我的学习，emmm要准备考研了 😵‍💫 </p><h3 id="简单介绍一下吧："><a href="#简单介绍一下吧：" class="headerlink" title="简单介绍一下吧："></a><strong><font color="red">简单介绍一下吧：</font></strong></h3><blockquote><p>我是<em>Yuejin Wu</em>, <em>BUPTer in AI</em> 🧑‍🎓</p><p>借这个博客，我将分享并同步我的学习，有太多的东西要学习：</p><ul><li>深度学习（代码好难写👨‍💻💔）</li><li>前后端开发（留条后路吧）</li><li><strong>English</strong>（我讨厌英语💩）</li><li><font color="green">考研考研考研</font> （祝我上岸吧，本校就好）🙏</li><li>再多的…下回分解吧</li></ul></blockquote><h3 id="我喜欢的："><a href="#我喜欢的：" class="headerlink" title="我喜欢的："></a><strong><font color="blue">我喜欢的：</font></strong></h3><p><strong>我喜欢打篮球 🏀</strong></p><blockquote><p>我是传奇球星<em>Kobe Bryant</em>的铁杆粉丝 </p><p><em>Kobe</em>粉丝的字典里没有”放弃“二字！</p><p><strong>“ Friends always come and go, but a champion flag will never fall ! 🏆 “</strong></p><div align=left><img src="https://n.sinaimg.cn/sinacn12/211/w2048h1363/20180823/15c8-hhzsnec6055235.jpg" alt="thegreat2010!" width="600" height="400" /></div><p><strong>“ Second place means you are the first loser ! “</strong></p><div align=left><img src="https://ts1.tc.mm.bing.net/th/id/R-C.f343e7606d3ee999bb7cca72021a3f25?rik=lnVs0YbiLAQp7A&riu=http%3a%2f%2ffile.qiumiwu.com%2fimgs%2f20200317%2f1584434559376554.jpg&ehk=4gVfPuk%2f8b4EryaVLHOjjjFlgAEaS0wuCz3XfHYua6I%3d&risl=&pid=ImgRaw&r=0" alt="thegreat2010!" width="600" height="400" /></div><p><strong>“ Somebody has to win, so why not be me? “</strong></p><div align=left><img src="https://ts1.tc.mm.bing.net/th/id/R-C.4ac3ba3869c279ad8620c3492d3e61eb?rik=45oONH9yVyeLaQ&riu=http%3a%2f%2fimg.mp.sohu.com%2fupload%2f20171219%2f8e635a7bd4cf4ed4ae4573be8b7f9a16_th.JPG&ehk=w%2bHL%2fvomMJ2jw8XXdj087b%2bix3UAgV4WenYhYnCVXEc%3d&risl=&pid=ImgRaw&r=0" alt="thegreat2010!" width="600" height="400" /></div><p><strong><font size=5><font color="purple">Mamba Out !</font><font color="orange">  Mamba Forever!</font></font></strong></p></blockquote><p><strong>我喜欢听的歌挺多的</strong></p><p>**歌剧音乐剧🎷、流行通俗🎸我都喜欢 **</p><p><strong>钟爱<em>Lana Del Ray</em></strong> </p><blockquote><p><img src="https://tse2-mm.cn.bing.net/th/id/OIP-C.QkE_dEvv2a_1rfpSvc4NLwAAAA?w=166&h=180&c=7&r=0&o=7&dpr=2.2&pid=1.7&rm=3" alt="lanadelray专辑 的图像结果" style="zoom: 68.5%;" /><img src="https://tse1-mm.cn.bing.net/th/id/OIP-C.-RL4R68dMgjYkp05BCqXWQHaHa?w=185&h=187&c=7&r=0&o=7&dpr=2.2&pid=1.7&rm=3 =" alt="lanadelray专辑 的图像结果" style="zoom:50%;" /></p><p><em><strong>2023年夏至今，百听不厌，YYDS</strong></em> 🥰</p></blockquote><h3 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a><font color="broen">未来规划</font></h3><p>emmm 好好干 好好学 🧘‍♂️</p><p>争取读个研，进大厂，争取干算法岗</p><p>好吧，现在是北京时间 00：29 ， 2025.12.10   </p><p>下次再聊，晚安~🤞 </p><div align=middle><img src="https://www.bupt.edu.cn/__local/C/8E/F7/EE902059AE32E0E6325EFEE8F46_B2D41D06_CD58.png" alt="thegreat2010!" width="600" height="150" /></div>]]></content>
      
      
      <categories>
          
          <category> about </category>
          
          <category> essay </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AI Platform部分技术栈</title>
      <link href="/2025/12/08/AIpiatform_%E6%8A%80%E6%9C%AF%E6%A0%88/"/>
      <url>/2025/12/08/AIpiatform_%E6%8A%80%E6%9C%AF%E6%A0%88/</url>
      
        <content type="html"><![CDATA[<p>以下是为我<strong>零基础新手</strong>量身定制的<strong>容器平台技术栈学习路线</strong>。</p><h2 id="🎯-零基础学习路线图（总览）"><a href="#🎯-零基础学习路线图（总览）" class="headerlink" title="🎯 零基础学习路线图（总览）"></a>🎯 <strong>零基础学习路线图（总览）</strong></h2><p><strong>预计周期：3-4个月（按每周10小时学习时间）</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">第一阶段：基础认知与Linux (1-2周)</span><br><span class="line">第二阶段：Docker核心技能 (3-4周)  </span><br><span class="line">第三阶段：存储与网络基础 (2-3周)</span><br><span class="line">第四阶段：平台实战与理解 (2-3周)</span><br><span class="line">第五阶段：扩展与进阶 (持续学习)</span><br></pre></td></tr></table></figure><hr><h2 id="📘-第一阶段：基础认知与Linux-1-2周"><a href="#📘-第一阶段：基础认知与Linux-1-2周" class="headerlink" title="📘 第一阶段：基础认知与Linux (1-2周)"></a>📘 <strong>第一阶段：基础认知与Linux (1-2周)</strong></h2><h3 id="目标：建立计算机基础认知，学会基本的Linux操作"><a href="#目标：建立计算机基础认知，学会基本的Linux操作" class="headerlink" title="目标：建立计算机基础认知，学会基本的Linux操作"></a><strong>目标</strong>：建立计算机基础认知，学会基本的Linux操作</h3><p><strong>学习内容</strong>：</p><ol><li><p><strong>计算机基础概念</strong></p><ul><li>什么是操作系统、进程、内存、硬盘</li><li>什么是IP地址、端口、网络协议</li><li>[B站搜索]：“计算机基础入门” 系列视频</li></ul></li><li><p><strong>Linux基础命令（最重要！）</strong></p><ul><li>如何在命令行中移动、查看文件</li><li>如何安装软件、管理权限</li><li><strong>推荐学习资源</strong>：<ul><li>[B站]：搜索“Linux基础命令10小时”</li><li>[网站]：菜鸟教程Linux教程</li><li><strong>动手练习</strong>：在Windows上安装WSL2，或Mac使用终端</li></ul></li></ul></li></ol><p><strong>每日任务示例</strong>：</p><ul><li>Day1：学习 <code>ls</code>, <code>cd</code>, <code>pwd</code> 命令</li><li>Day2：学习 <code>mkdir</code>, <code>rm</code>, <code>cp</code> 命令</li><li>Day3：学习 <code>cat</code>, <code>vim</code>, <code>nano</code> 编辑文件</li><li>Day4：学习 <code>ps</code>, <code>top</code> 查看进程</li><li>Day5：学习 <code>chmod</code>, <code>chown</code> 权限管理</li></ul><hr><h2 id="🐳-第二阶段：Docker核心技能-3-4周"><a href="#🐳-第二阶段：Docker核心技能-3-4周" class="headerlink" title="🐳 第二阶段：Docker核心技能 (3-4周)"></a>🐳 <strong>第二阶段：Docker核心技能 (3-4周)</strong></h2><h3 id="目标：掌握Docker基本使用，理解容器概念"><a href="#目标：掌握Docker基本使用，理解容器概念" class="headerlink" title="目标：掌握Docker基本使用，理解容器概念"></a><strong>目标</strong>：掌握Docker基本使用，理解容器概念</h3><p><strong>学习路径</strong>：</p><ol><li><p><strong>Docker是什么？（第1周）</strong></p><ul><li>看这个视频：[B站]“Docker通俗易懂解释”</li><li>安装Docker Desktop（Windows&#x2F;Mac一键安装）</li><li>运行第一个容器：<code>docker run hello-world</code></li></ul></li><li><p><strong>Docker基础操作（第2周）</strong></p><ul><li>学习核心命令：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker pull     # 下载镜像</span><br><span class="line">docker run      # 运行容器</span><br><span class="line">docker ps       # 查看运行中的容器</span><br><span class="line">docker stop     # 停止容器</span><br><span class="line">docker rm       # 删除容器</span><br><span class="line">docker images   # 查看镜像</span><br></pre></td></tr></table></figure></li><li>完成一个小练习：用Docker运行一个Nginx网站</li></ul></li><li><p><strong>Dockerfile与镜像制作（第3周）</strong></p><ul><li>理解镜像和容器的关系</li><li>学习编写简单的Dockerfile</li><li>自己制作一个包含Python环境的镜像</li></ul></li><li><p><strong>数据持久化（第4周）</strong></p><ul><li>理解Volume的概念</li><li>学习将本地文件夹挂载到容器内</li><li>练习：创建一个MySQL容器，将数据保存在本地</li></ul></li></ol><p><strong>推荐资源</strong>：</p><ul><li>[视频]：B站“Docker入门到实战”</li><li>[交互式学习]：Docker官方教程（有中文）</li><li>[书籍]：《Docker入门与实践》（电子版）</li></ul><hr><h2 id="💾-第三阶段：存储与网络基础-2-3周"><a href="#💾-第三阶段：存储与网络基础-2-3周" class="headerlink" title="💾 第三阶段：存储与网络基础 (2-3周)"></a>💾 <strong>第三阶段：存储与网络基础 (2-3周)</strong></h2><h3 id="目标：理解平台中的存储和网络设计"><a href="#目标：理解平台中的存储和网络设计" class="headerlink" title="目标：理解平台中的存储和网络设计"></a><strong>目标</strong>：理解平台中的存储和网络设计</h3><p><strong>学习内容</strong>：</p><ol><li><p><strong>NFS基础（第1周）</strong></p><ul><li>什么是NFS（网络文件系统）</li><li>为什么需要共享存储</li><li>简单搭建一个NFS服务体验</li></ul></li><li><p><strong>理解平台存储架构</strong></p><ul><li>重新阅读文档中的“容器磁盘 vs NFS云盘”</li><li>画图理解：容器内部 <code>/root</code> 目录是如何挂载NFS的</li><li>理解为什么conda环境可以共享</li></ul></li><li><p><strong>网络基础（第2-3周）</strong></p><ul><li>什么是IP、端口、网关</li><li>Docker网络基础：桥接网络、端口映射</li><li>理解 <code>-p 8080:80</code> 是什么意思</li><li>学习简单的网络故障排查</li></ul></li></ol><p><strong>动手实验</strong>：</p><ul><li>在本地搭建两个虚拟机，配置NFS共享</li><li>创建一个Docker容器，挂载NFS共享目录</li></ul><hr><h2 id="🏗️-第四阶段：平台实战与理解-2-3周"><a href="#🏗️-第四阶段：平台实战与理解-2-3周" class="headerlink" title="🏗️ 第四阶段：平台实战与理解 (2-3周)"></a>🏗️ <strong>第四阶段：平台实战与理解 (2-3周)</strong></h2><h3 id="目标：将所学应用到实际平台理解中"><a href="#目标：将所学应用到实际平台理解中" class="headerlink" title="目标：将所学应用到实际平台理解中"></a><strong>目标</strong>：将所学应用到实际平台理解中</h3><p><strong>学习步骤</strong>：</p><ol><li><p><strong>平台架构图绘制</strong></p><ul><li>根据文档，画出平台的简化架构图</li><li>标记出：用户 → Web界面 → Gateway → Host → Container → NFS</li><li>理解数据流向</li></ul></li><li><p><strong>实际操作平台</strong></p><ul><li>申请一个测试容器</li><li>在容器内安装软件（用apt）</li><li>在&#x2F;root目录下创建文件，理解NFS共享</li><li>用conda创建一个Python环境</li></ul></li><li><p><strong>资源监控初识</strong></p><ul><li>学习使用 <code>htop</code>, <code>nvidia-smi</code>, <code>df -h</code> 等命令</li><li>理解平台文档中的“CPU、内存、GPU监控”</li></ul></li><li><p><strong>问题排查基础</strong></p><ul><li>容器启动失败怎么办？</li><li>网络不通怎么办？</li><li>磁盘空间不足怎么办？</li></ul></li></ol><hr><h2 id="🚀-第五阶段：扩展与进阶-持续学习"><a href="#🚀-第五阶段：扩展与进阶-持续学习" class="headerlink" title="🚀 第五阶段：扩展与进阶 (持续学习)"></a>🚀 <strong>第五阶段：扩展与进阶 (持续学习)</strong></h2><h3 id="目标：深入理解平台高级特性"><a href="#目标：深入理解平台高级特性" class="headerlink" title="目标：深入理解平台高级特性"></a><strong>目标</strong>：深入理解平台高级特性</h3><p><strong>可选学习方向</strong>：</p><ol><li><strong>容器编排</strong>：学习Docker Compose、Kubernetes基础</li><li><strong>监控系统</strong>：学习Prometheus + Grafana</li><li><strong>CI&#x2F;CD</strong>：学习GitLab CI或GitHub Actions</li><li><strong>安全基础</strong>：学习容器安全、权限控制</li></ol><hr><h2 id="📚-学习资源推荐（零基础友好）"><a href="#📚-学习资源推荐（零基础友好）" class="headerlink" title="📚 学习资源推荐（零基础友好）"></a>📚 <strong>学习资源推荐（零基础友好）</strong></h2><h3 id="视频教程（B站）："><a href="#视频教程（B站）：" class="headerlink" title="视频教程（B站）："></a><strong>视频教程（B站）</strong>：</h3><ol><li>【Linux】“Linux基础命令全集” - 黑马程序员</li><li>【Docker】“Docker容器技术全集” - 尚硅谷</li><li>【网络】“计算机网络基础” - 湖科大教书匠</li></ol><h3 id="网站-文档："><a href="#网站-文档：" class="headerlink" title="网站&#x2F;文档："></a><strong>网站&#x2F;文档</strong>：</h3><ol><li><strong>菜鸟教程</strong>（runoob.com）- Linux&#x2F;Docker基础</li><li><strong>Docker官方文档</strong>（有中文）- 最权威</li><li><strong>MDN Web文档</strong> - 学习网络基础知识</li></ol><h3 id="实践平台："><a href="#实践平台：" class="headerlink" title="实践平台："></a><strong>实践平台</strong>：</h3><ol><li><strong>本地</strong>：Docker Desktop + WSL2</li><li><strong>在线实验</strong>：Katacoda（免费Docker实验环境）</li><li><strong>云服务器</strong>：阿里云&#x2F;腾讯云学生优惠（最低配置即可）</li></ol><hr><h2 id="🎯-具体学习计划表（前2个月）"><a href="#🎯-具体学习计划表（前2个月）" class="headerlink" title="🎯 具体学习计划表（前2个月）"></a>🎯 <strong>具体学习计划表（前2个月）</strong></h2><table><thead><tr><th>周数</th><th>学习重点</th><th>每日任务（约1-2小时）</th><th>完成标志</th></tr></thead><tbody><tr><td>第1周</td><td>Linux基础</td><td>学习5个命令+练习</td><td>能在命令行中自如移动、查看文件</td></tr><tr><td>第2周</td><td>Docker安装与运行</td><td>安装Docker，运行3种不同容器</td><td>成功运行Nginx、Redis、MySQL容器</td></tr><tr><td>第3周</td><td>Docker命令精通</td><td>掌握10个核心命令</td><td>能独立管理容器生命周期</td></tr><tr><td>第4周</td><td>Dockerfile实践</td><td>编写2个Dockerfile</td><td>成功构建自定义镜像</td></tr><tr><td>第5周</td><td>数据持久化</td><td>练习Volume挂载</td><td>实现容器数据持久化保存</td></tr><tr><td>第6周</td><td>NFS基础</td><td>搭建简单NFS服务</td><td>实现两台机器文件共享</td></tr><tr><td>第7周</td><td>平台理解</td><td>画出平台架构图</td><td>能向别人解释平台工作原理</td></tr><tr><td>第8周</td><td>平台实战</td><td>在平台上完成一个任务</td><td>成功创建容器、安装软件、保存数据</td></tr></tbody></table><hr><h2 id="❓-遇到困难怎么办？"><a href="#❓-遇到困难怎么办？" class="headerlink" title="❓ 遇到困难怎么办？"></a>❓ <strong>遇到困难怎么办？</strong></h2><ol><li><strong>谷歌&#x2F;百度是你的朋友</strong>：搜索“docker如何xxx”</li><li><strong>加入技术社区</strong>：Stack Overflow、SegmentFault、知乎</li><li><strong>问问题技巧</strong>：描述你做了什么、期望结果是什么、实际结果是什么、错误信息是什么</li><li><strong>先尝试再问</strong>：自己尝试解决30分钟，解决不了再问</li></ol><hr><h2 id="💡-最重要建议"><a href="#💡-最重要建议" class="headerlink" title="💡 最重要建议"></a>💡 <strong>最重要建议</strong></h2><ol><li><strong>动手 &gt; 看视频 &gt; 看书</strong>：技术学习必须动手</li><li><strong>不要怕犯错</strong>：容器可以随意删除重建</li><li><strong>从简单开始</strong>：先跑起来，再理解原理</li><li><strong>定期复习</strong>：每周回顾一次学过的内容</li><li><strong>找同伴</strong>：找个一起学习的伙伴互相督促</li></ol><p><strong>现在就开始</strong>：今天就在你的电脑上安装Docker，运行 <code>docker run hello-world</code>！</p>]]></content>
      
      
      <categories>
          
          <category> 开发 </category>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> plan </tag>
            
            <tag> AI Platform </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>考研英语Plans</title>
      <link href="/2025/12/07/English/"/>
      <url>/2025/12/07/English/</url>
      
        <content type="html"><![CDATA[<h2 id="总目标：2027考研"><a href="#总目标：2027考研" class="headerlink" title="总目标：2027考研"></a>总目标：2027考研</h2><p>目标院校</p><blockquote><p><strong>本校就好</strong></p></blockquote><p>目标分数</p><blockquote><p>英语别拖累我就好</p></blockquote><h2 id="基本规划"><a href="#基本规划" class="headerlink" title="基本规划"></a>基本规划</h2><h3 id="跨年之前"><a href="#跨年之前" class="headerlink" title="跨年之前"></a>跨年之前</h3><ul><li>多看看外刊 </li><li>多听听外语</li><li>多背背单词</li></ul><h3 id="跨年之后"><a href="#跨年之后" class="headerlink" title="跨年之后"></a>跨年之后</h3><ul><li>该刷题了</li></ul>]]></content>
      
      
      <categories>
          
          <category> English </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
            <tag> plan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一句话在这！</title>
      <link href="/2025/12/06/THE%20FIRST/"/>
      <url>/2025/12/06/THE%20FIRST/</url>
      
        <content type="html"><![CDATA[<h1 id="第一句话"><a href="#第一句话" class="headerlink" title="第一句话"></a>第一句话</h1><p>能看见我吗？！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/12/06/hello-world/"/>
      <url>/2025/12/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>这个是我留的礼物！！！</p><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
